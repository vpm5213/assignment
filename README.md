# E-Commerce Data Pipeline

This project implements an ETL pipeline using Apache Airflow to process the Brazilian E-Commerce Public Dataset by Olist. The pipeline extracts data from CSV files, loads it into a PostgreSQL database, computes business KPIs, and visualizes the results.

## Project Overview

This data pipeline processes e-commerce transactional data to extract meaningful business insights through the following components:

1. **Data Acquisition Pipeline**: Extracts data from CSV files and loads it into PostgreSQL
2. **Data Transformation Pipeline**: Computes business KPIs and generates visualizations
3. **Containerized Environment**: Uses Docker to ensure consistent deployment

## Dataset

The [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce) contains approximately 100,000 orders from 2016 to 2018 made at multiple marketplaces in Brazil. It includes information on:

- Orders
- Products
- Customers
- Reviews
- Sellers
- Geolocation

## KPIs Implemented

1. **Customer Lifetime Value (CLV)**: Measures the total revenue generated by customers over their lifetime with the business.
   - Business relevance: Helps identify high-value customers and optimize customer acquisition costs.

2. **Regional Sales Performance**: Analyzes sales patterns across different Brazilian states and cities.
   - Business relevance: Enables targeted marketing strategies and supply chain optimization for high-performing regions.

3. **Product Category Performance**: Evaluates which product categories generate the most revenue and have the highest growth.
   - Business relevance: Guides inventory decisions and helps identify trending product categories.

## Technical Implementation

### Environment Setup

The project uses:
- Python 3.9+
- Apache Airflow 2.7.1
- PostgreSQL 13
- Docker & Docker Compose

### Directory Structure

```
project/
├── airflow/                # Airflow configuration
│   └── Dockerfile
├── dags/                   # Airflow DAGs
│   ├── data_acquisition_dag.py
│   └── data_transformation_dag.py
├── data/                   # Dataset storage
├── docker-compose.yml      # Docker configuration
├── requirements.txt        # Python dependencies
└── README.md
```

## Setup Instructions

### Prerequisites

- Docker and Docker Compose installed
- Git
- Kaggle account for dataset access

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/vpm5213/assignment
   cd ecommerce-data-pipeline
   ```

2. Start the Docker containers:
   ```bash
   docker-compose up -d
   ```

3. Access Airflow web interface:
   ```
   http://localhost:8080
   ```
   - Default username: admin
   - Default password: admin

4. Trigger the data acquisition DAG manually or wait for scheduled execution

### Running the Pipeline

1. The data acquisition DAG will:
   - Download the dataset from Kaggle (if not already present)
   - Create database schema
   - Load CSV data into PostgreSQL tables
   - Perform initial data validation

2. The data transformation DAG will:
   - Compute the defined KPIs
   - Store results in dedicated tables
   - Generate visualizations

## LLM Assistance Used

I used Claude to help with various aspects of this project:

### Effective Prompts

1. **Docker Setup**: "Help me create a docker-compose.yml file for a data pipeline using Airflow and PostgreSQL."
   - This helped me properly configure the containerized environment with the necessary services.

2. **DAG Structure**: "Show me how to create an Airflow DAG that extracts data from CSV files and loads it into PostgreSQL."
   - Claude provided the basic structure and best practices for Airflow DAG creation.

3. **SQL Query Design**: "Help me write SQL queries to calculate customer lifetime value from e-commerce transaction data."
   - This helped me develop efficient queries for KPI calculation.

### Areas Where LLM Was Insufficient

1. **Local Environment Troubleshooting**: I had to manually debug Docker networking issues that were specific to my setup.


## Challenges and Solutions

### Challenge 1: Data Volume
**Problem**: The dataset contains millions of rows across multiple tables, causing performance issues.
**Solution**: Implemented incremental loading and optimized PostgreSQL configuration for bulk operations.

### Challenge 2: Data Quality Issues
**Problem**: Missing values and inconsistencies in the original dataset.
**Solution**: Added data validation steps in the acquisition DAG with automated error logging and alerting.

### Challenge 3: DAG Dependencies
**Problem**: Complex dependencies between transformation tasks led to scheduling issues.
**Solution**: Redesigned the DAG structure using task groups and improved dependency management with XComs.

## Future Improvements

- Implement real-time dashboarding with Grafana
- Add machine learning models for customer segmentation and churn prediction
- Enhance the pipeline with data quality monitoring
- Implement CI/CD for automated testing and deployment

## Acknowledgments

- Olist for providing the public dataset
- Apache Airflow community for documentation and examples